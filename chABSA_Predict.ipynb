{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e9de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/anaconda3/envs/pytorch/lib/python3.6')\n",
    "sys.path.append('/opt/anaconda3/envs/pytorch/lib/python3.6/site-packages')\n",
    "sys.path.append('/opt/anaconda3/envs/pytorch/lib/python3.6/site-packages/appv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec55826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6cd601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import get_chABSA_DataLoaders_and_TEXT\n",
    "from utils.bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea5a434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "見出し:今夜, 読み:こんや, 原形:今夜, 品詞:名詞, 品詞細分類:時相名詞, 活用型:*, 活用形:*, 意味情報:代表表記:今夜/こんや カテゴリ:時間, 代表表記:今夜/こんや\n",
      "見出し:の, 読み:の, 原形:の, 品詞:助詞, 品詞細分類:接続助詞, 活用型:*, 活用形:*, 意味情報:NIL, 代表表記:\n",
      "見出し:月, 読み:げつ, 原形:月, 品詞:名詞, 品詞細分類:時相名詞, 活用型:*, 活用形:*, 意味情報:代表表記:月/げつ 漢字読み:音 カテゴリ:時間, 代表表記:月/げつ\n",
      "見出し:は, 読み:は, 原形:は, 品詞:助詞, 品詞細分類:副助詞, 活用型:*, 活用形:*, 意味情報:NIL, 代表表記:\n",
      "見出し:綺麗です, 読み:きれいです, 原形:綺麗だ, 品詞:形容詞, 品詞細分類:*, 活用型:ナ形容詞, 活用形:デス列基本形, 意味情報:代表表記:綺麗だ/きれいだ, 代表表記:綺麗だ/きれいだ\n",
      "見出し:ね, 読み:ね, 原形:ね, 品詞:助詞, 品詞細分類:終助詞, 活用型:*, 活用形:*, 意味情報:NIL, 代表表記:\n",
      "見出し:。, 読み:。, 原形:。, 品詞:特殊, 品詞細分類:句点, 活用型:*, 活用形:*, 意味情報:NIL, 代表表記:\n"
     ]
    }
   ],
   "source": [
    "# JUMANの動作確認\n",
    "from pyknp import Juman\n",
    "juman = Juman()\n",
    "result = juman.analysis(\"今夜の月は綺麗ですね。\")\n",
    "for mrph in result.mrph_list(): # 各形態素にアクセス\n",
    "    print(\"見出し:%s, 読み:%s, 原形:%s, 品詞:%s, 品詞細分類:%s, 活用型:%s, 活用形:%s, 意味情報:%s, 代表表記:%s\" \\\n",
    "            % (mrph.midasi, mrph.yomi, mrph.genkei, mrph.hinsi, mrph.bunrui, mrph.katuyou1, mrph.katuyou2, mrph.imis, mrph.repname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c3b2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "from predict import predict, create_vocab_text, build_bert_model\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0564a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['その', '一方', 'で', '、', '中国', '経済', 'の', '景気', '減速', 'や', '米国', '新', '政権', 'の', '政策', '運営', '、', '英国', 'の', 'ＥＵ', '離脱', '等', 'の', 'リスク', 'に', 'より', '、', '[UNK]', 'は', '依然と', 'して', '[UNK]', '状況', 'に', 'あり', 'ます']\n",
      "['その', '一方', 'で', '、', '中国', '経済', 'の', '景気', '減速', 'や', '米国', '新', '政権', 'の', '政策', '運営', '、', '英国', 'の', 'ＥＵ', '離脱', '等', 'の', 'リスク', 'に', 'より', '、', '[UNK]', 'は', '依然と', 'して', '[UNK]', '状況', 'に', 'あり', 'ます']\n"
     ]
    }
   ],
   "source": [
    "TEXT = create_vocab_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549fa6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight→embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight→embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight→embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight→embeddings.LayerNorm.gamma\n",
      "bert.embeddings.LayerNorm.bias→embeddings.LayerNorm.beta\n",
      "bert.encoder.layer.0.attention.self.query.weight→encoder.layer.0.attention.selfattn.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias→encoder.layer.0.attention.selfattn.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight→encoder.layer.0.attention.selfattn.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias→encoder.layer.0.attention.selfattn.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight→encoder.layer.0.attention.selfattn.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias→encoder.layer.0.attention.selfattn.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight→encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias→encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight→encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias→encoder.layer.0.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.0.intermediate.dense.weight→encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias→encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight→encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias→encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight→encoder.layer.0.output.LayerNorm.gamma\n",
      "bert.encoder.layer.0.output.LayerNorm.bias→encoder.layer.0.output.LayerNorm.beta\n",
      "bert.encoder.layer.1.attention.self.query.weight→encoder.layer.1.attention.selfattn.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias→encoder.layer.1.attention.selfattn.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight→encoder.layer.1.attention.selfattn.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias→encoder.layer.1.attention.selfattn.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight→encoder.layer.1.attention.selfattn.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias→encoder.layer.1.attention.selfattn.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight→encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias→encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight→encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias→encoder.layer.1.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.1.intermediate.dense.weight→encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias→encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight→encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias→encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight→encoder.layer.1.output.LayerNorm.gamma\n",
      "bert.encoder.layer.1.output.LayerNorm.bias→encoder.layer.1.output.LayerNorm.beta\n",
      "bert.encoder.layer.2.attention.self.query.weight→encoder.layer.2.attention.selfattn.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias→encoder.layer.2.attention.selfattn.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight→encoder.layer.2.attention.selfattn.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias→encoder.layer.2.attention.selfattn.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight→encoder.layer.2.attention.selfattn.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias→encoder.layer.2.attention.selfattn.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight→encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias→encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight→encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias→encoder.layer.2.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.2.intermediate.dense.weight→encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias→encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight→encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias→encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight→encoder.layer.2.output.LayerNorm.gamma\n",
      "bert.encoder.layer.2.output.LayerNorm.bias→encoder.layer.2.output.LayerNorm.beta\n",
      "bert.encoder.layer.3.attention.self.query.weight→encoder.layer.3.attention.selfattn.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias→encoder.layer.3.attention.selfattn.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight→encoder.layer.3.attention.selfattn.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias→encoder.layer.3.attention.selfattn.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight→encoder.layer.3.attention.selfattn.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias→encoder.layer.3.attention.selfattn.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight→encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias→encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight→encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias→encoder.layer.3.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.3.intermediate.dense.weight→encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias→encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight→encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias→encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight→encoder.layer.3.output.LayerNorm.gamma\n",
      "bert.encoder.layer.3.output.LayerNorm.bias→encoder.layer.3.output.LayerNorm.beta\n",
      "bert.encoder.layer.4.attention.self.query.weight→encoder.layer.4.attention.selfattn.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias→encoder.layer.4.attention.selfattn.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight→encoder.layer.4.attention.selfattn.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias→encoder.layer.4.attention.selfattn.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight→encoder.layer.4.attention.selfattn.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias→encoder.layer.4.attention.selfattn.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight→encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias→encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight→encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias→encoder.layer.4.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.4.intermediate.dense.weight→encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias→encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight→encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias→encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight→encoder.layer.4.output.LayerNorm.gamma\n",
      "bert.encoder.layer.4.output.LayerNorm.bias→encoder.layer.4.output.LayerNorm.beta\n",
      "bert.encoder.layer.5.attention.self.query.weight→encoder.layer.5.attention.selfattn.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias→encoder.layer.5.attention.selfattn.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight→encoder.layer.5.attention.selfattn.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias→encoder.layer.5.attention.selfattn.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight→encoder.layer.5.attention.selfattn.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias→encoder.layer.5.attention.selfattn.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight→encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias→encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight→encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias→encoder.layer.5.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.5.intermediate.dense.weight→encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias→encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight→encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias→encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight→encoder.layer.5.output.LayerNorm.gamma\n",
      "bert.encoder.layer.5.output.LayerNorm.bias→encoder.layer.5.output.LayerNorm.beta\n",
      "bert.encoder.layer.6.attention.self.query.weight→encoder.layer.6.attention.selfattn.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias→encoder.layer.6.attention.selfattn.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight→encoder.layer.6.attention.selfattn.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias→encoder.layer.6.attention.selfattn.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight→encoder.layer.6.attention.selfattn.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias→encoder.layer.6.attention.selfattn.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight→encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias→encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight→encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias→encoder.layer.6.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.6.intermediate.dense.weight→encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias→encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight→encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias→encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight→encoder.layer.6.output.LayerNorm.gamma\n",
      "bert.encoder.layer.6.output.LayerNorm.bias→encoder.layer.6.output.LayerNorm.beta\n",
      "bert.encoder.layer.7.attention.self.query.weight→encoder.layer.7.attention.selfattn.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias→encoder.layer.7.attention.selfattn.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight→encoder.layer.7.attention.selfattn.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias→encoder.layer.7.attention.selfattn.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight→encoder.layer.7.attention.selfattn.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias→encoder.layer.7.attention.selfattn.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight→encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias→encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight→encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias→encoder.layer.7.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.7.intermediate.dense.weight→encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias→encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight→encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias→encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight→encoder.layer.7.output.LayerNorm.gamma\n",
      "bert.encoder.layer.7.output.LayerNorm.bias→encoder.layer.7.output.LayerNorm.beta\n",
      "bert.encoder.layer.8.attention.self.query.weight→encoder.layer.8.attention.selfattn.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias→encoder.layer.8.attention.selfattn.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight→encoder.layer.8.attention.selfattn.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias→encoder.layer.8.attention.selfattn.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight→encoder.layer.8.attention.selfattn.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias→encoder.layer.8.attention.selfattn.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight→encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias→encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight→encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias→encoder.layer.8.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.8.intermediate.dense.weight→encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias→encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight→encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias→encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight→encoder.layer.8.output.LayerNorm.gamma\n",
      "bert.encoder.layer.8.output.LayerNorm.bias→encoder.layer.8.output.LayerNorm.beta\n",
      "bert.encoder.layer.9.attention.self.query.weight→encoder.layer.9.attention.selfattn.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias→encoder.layer.9.attention.selfattn.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight→encoder.layer.9.attention.selfattn.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias→encoder.layer.9.attention.selfattn.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight→encoder.layer.9.attention.selfattn.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias→encoder.layer.9.attention.selfattn.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight→encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias→encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight→encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias→encoder.layer.9.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.9.intermediate.dense.weight→encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias→encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight→encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias→encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight→encoder.layer.9.output.LayerNorm.gamma\n",
      "bert.encoder.layer.9.output.LayerNorm.bias→encoder.layer.9.output.LayerNorm.beta\n",
      "bert.encoder.layer.10.attention.self.query.weight→encoder.layer.10.attention.selfattn.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias→encoder.layer.10.attention.selfattn.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight→encoder.layer.10.attention.selfattn.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias→encoder.layer.10.attention.selfattn.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight→encoder.layer.10.attention.selfattn.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias→encoder.layer.10.attention.selfattn.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight→encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias→encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight→encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias→encoder.layer.10.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.10.intermediate.dense.weight→encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias→encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight→encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias→encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight→encoder.layer.10.output.LayerNorm.gamma\n",
      "bert.encoder.layer.10.output.LayerNorm.bias→encoder.layer.10.output.LayerNorm.beta\n",
      "bert.encoder.layer.11.attention.self.query.weight→encoder.layer.11.attention.selfattn.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias→encoder.layer.11.attention.selfattn.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight→encoder.layer.11.attention.selfattn.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias→encoder.layer.11.attention.selfattn.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight→encoder.layer.11.attention.selfattn.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias→encoder.layer.11.attention.selfattn.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight→encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias→encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight→encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias→encoder.layer.11.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.11.intermediate.dense.weight→encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias→encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight→encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias→encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight→encoder.layer.11.output.LayerNorm.gamma\n",
      "bert.encoder.layer.11.output.LayerNorm.bias→encoder.layer.11.output.LayerNorm.beta\n",
      "bert.pooler.dense.weight→pooler.dense.weight\n",
      "bert.pooler.dense.bias→pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "from utils.bert import get_config, BertModel,BertForchABSA, set_learned_params\n",
    "\n",
    "from utils.bert import get_config, BertModel,BertForchABSA, set_learned_params\n",
    "\n",
    "# The setting parameter has been saved in config.json\n",
    "config = get_config(file_path=\"/opt/anaconda3/envs/pytorch/lib/python3.6/site-packages/appv1/weights/bert_config.json\")\n",
    "\n",
    "# build a BERT model\n",
    "net_bert = BertModel(config)\n",
    "\n",
    "# Trained BERT model params setting\n",
    "net_bert = set_learned_params(\n",
    "    net_bert, weights_path=\"/opt/anaconda3/envs/pytorch/lib/python3.6/site-packages/appv1/weights/pytorch_model.bin\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2302aefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForchABSA(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32006, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the finetuning module\n",
    "net_trained = BertForchABSA(net_bert)\n",
    "save_path = '/opt/anaconda3/envs/pytorch/lib/python3.6/site-packages/appv1/weights/bert_fine_tuning_chABSA.pth'   \n",
    "\n",
    "net_trained.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "net_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf4fb6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['当社', 'の', '業績', 'は', '多岐に', 'わたる', '変動', '要因', 'の', '影響', 'を', '受ける', '可能', '性', 'が', 'あり', 'ます', '。', '有価', '証券', '報告', '書', 'に', '記載', 'した', '事業', 'の', '状況', '、', '経理', 'の', '状況', '等', 'に', '関する', '事項', 'の', 'うち', '、', '投資', '者', 'の', '判断', 'に', '重要な', '影響', 'を', '及', 'ぼ', 'す', '可', '能', '性', 'の', 'あ', 'る', '事', '項', 'に', 'は', '、', '以下', 'の', 'ような', 'もの', 'が', 'あり', 'ます', '。', 'なお', '、', '[UNK]', 'に', 'おける', '将来', 'に', '関する', '事項', 'は', '、', '当', '事業', '年度', '末', '現在', 'に', 'おいて', '当社', 'が', '判断', 'した', 'もの', 'であり', 'ます', '。', '（', '[UNK]', '）', '主要', '市場', 'で', 'の', '需要', 'の', '急激な', '変動', 'に', 'ついて', '当社', 'は', '、', '主に', '半導体', '業界', '及び', '[UNK]', '業界', 'を', '対象', 'と', 'して', '、', 'その', '生産', 'ライン', 'で', '用い', 'られる', '各種', '生産', '設備', '部品', 'の', '製造', '・', '販売', 'を', '行って', 'い', 'ます', 'が', '、', '半導体', '業界', 'に', 'おき', 'まして', 'シリコン', 'サイクル', '、', '[UNK]', '業界', 'に', 'おき', 'まして', 'クリスタル', 'サイクル', 'と', '呼ば', 'れる', '業界', '特有の', '好', '不況', 'の', '波', 'が', '存在', 'し', 'ます', '。', '当社', 'に', 'おき', 'まして', 'は', '、', 'メーカー', 'の', '設備', '投資', '動向', 'に', '左右さ', 'れ', 'ない', '消耗', '品', 'など', 'の', '安定', '的な', '販売', 'が', '[UNK]', '分野', 'の', '受注', 'に', '注力', 'する', 'など', 'の', '対策', 'を', '行い', '、', '業績', 'へ', 'の', '影響', 'を', '最小', '限', 'に', 'す', 'べく', '努力', 'して', 'おり', 'ます', '。', 'しかしながら', '、', 'これ', 'ら', 'の', '景気', '変動', 'に', 'よって', '、', '当社', 'の', '業績', '及び', '財務', '状況', 'に', '影響', 'を', '与える', '可能', '性', 'が', 'あり', 'ます', '。']\n",
      "[2, 11466, 5, 5356, 9, 10988, 5032, 5577, 4486, 5, 343, 10, 1164, 446, 108, 11, 125, 1953, 7, 27042, 4190, 1036, 520, 8, 1427, 20, 318, 5, 919, 6, 18270, 5, 919, 174, 8, 492, 6109, 5, 381, 6, 2869, 57, 5, 1843, 8, 1186, 343, 10, 14757, 7988, 954, 4261, 4636, 108, 5, 1208, 3282, 270, 1235, 8, 9, 6, 200, 5, 255, 60, 11, 125, 1953, 7, 194, 6, 1, 8, 217, 3253, 8, 492, 6109, 9, 6, 719, 318, 594, 562, 95, 8, 117, 11466, 11, 1843, 20, 60, 103, 1953, 7, 16, 1, 17, 1681, 1344, 13, 5, 3988, 5, 16137, 5577, 8, 130, 11466, 9, 6, 542, 10992, 3197, 388, 1, 3197, 10, 703, 12, 19, 6, 58, 506, 1364, 13, 519, 124, 3181, 506, 2219, 3905, 5, 487, 15, 409, 10, 462, 142, 1953, 11, 6, 10992, 3197, 8, 7779, 26207, 23571, 7372, 6, 1, 3197, 8, 7779, 26207, 12734, 7372, 12, 180, 41, 3197, 9453, 2542, 11958, 5, 1639, 11, 162, 31, 1953, 7, 11466, 8, 7779, 26207, 9, 6, 1327, 5, 2219, 2869, 9824, 8, 19707, 54, 47, 15016, 695, 42, 5, 1847, 96, 409, 11, 1, 1300, 5, 13390, 8, 20182, 22, 42, 5, 2015, 10, 845, 6, 5356, 61, 5, 343, 10, 6983, 5682, 8, 954, 3011, 5693, 19, 79, 1953, 7, 3276, 6, 73, 94, 5, 9987, 5577, 8, 86, 6, 11466, 5, 5356, 388, 5540, 919, 8, 343, 10, 2377, 446, 108, 11, 125, 1953, 7, 3]\n",
      "======================推論結果の表示======================\n",
      "当社の業績は多岐にわたる変動要因の影響を受ける可能性があります。有価証券報告書に記載した事業の状況、経理の状況等に関する事項のうち、投資者の判断に重要な影響を及ぼす可能性のある事項には、以下のようなものがあります。なお、文中における将来に関する事項は、当事業年度末現在において当社が判断したものであります。（１）主要市場での需要の急激な変動について当社は、主に半導体業界及びＦＰＤ業界を対象として、その生産ラインで用いられる各種生産設備部品の製造・販売を行っていますが、半導体業界におきましてシリコンサイクル、ＦＰＤ業界におきましてクリスタルサイクルと呼ばれる業界特有の好不況の波が存在します。当社におきましては、メーカーの設備投資動向に左右されない消耗品などの安定的な販売が見込める分野の受注に注力するなどの対策を行い、業績への影響を最小限にすべく努力しております。しかしながら、これらの景気変動によって、当社の業績及び財務状況に影響を与える可能性があります。\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "推論ラベル：Negative<br><br>[BERTのAttentionを可視化_ALL]<br><span style=\"background-color: #FFFEFE\"> [CLS]</span><span style=\"background-color: #FFCCCC\"> 当社</span><span style=\"background-color: #FFFAFA\"> の</span><span style=\"background-color: #FFB5B5\"> 業績</span><span style=\"background-color: #FFF3F3\"> は</span><span style=\"background-color: #FFF1F1\"> 多岐に</span><span style=\"background-color: #FFFEFE\"> わたる</span><span style=\"background-color: #FFC9C9\"> 変動</span><span style=\"background-color: #FFE3E3\"> 要因</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFBABA\"> 影響</span><span style=\"background-color: #FFFEFE\"> を</span><span style=\"background-color: #FFCBCB\"> 受ける</span><span style=\"background-color: #FFE8E8\"> 可能</span><span style=\"background-color: #FFA2A2\"> 性</span><span style=\"background-color: #FFFBFB\"> が</span><span style=\"background-color: #FFD4D4\"> あり</span><span style=\"background-color: #FFD9D9\"> ます</span><span style=\"background-color: #FFD0D0\"> 。</span><span style=\"background-color: #FFF0F0\"> 有価</span><span style=\"background-color: #FFF9F9\"> 証券</span><span style=\"background-color: #FFFCFC\"> 報告</span><span style=\"background-color: #FFF7F7\"> 書</span><span style=\"background-color: #FFFEFE\"> に</span><span style=\"background-color: #FFFBFB\"> 記載</span><span style=\"background-color: #FFFDFD\"> した</span><span style=\"background-color: #FFF6F6\"> 事業</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFEEEE\"> 状況</span><span style=\"background-color: #FFFEFE\"> 、</span><span style=\"background-color: #FFFAFA\"> 経理</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFFBFB\"> 状況</span><span style=\"background-color: #FFE9E9\"> 等</span><span style=\"background-color: #FFFEFE\"> に</span><span style=\"background-color: #FFF1F1\"> 関する</span><span style=\"background-color: #FFA1A1\"> 事項</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFFDFD\"> うち</span><span style=\"background-color: #FFFEFE\"> 、</span><span style=\"background-color: #FFA3A3\"> 投資</span><span style=\"background-color: #FFFDFD\"> 者</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFF8F8\"> 判断</span><span style=\"background-color: #FFFEFE\"> に</span><span style=\"background-color: #FFFDFD\"> 重要な</span><span style=\"background-color: #FFBBBB\"> 影響</span><span style=\"background-color: #FFFEFE\"> を</span><span style=\"background-color: #FFFDFD\"> 及</span><span style=\"background-color: #FFFCFC\"> ぼ</span><span style=\"background-color: #FFF9F9\"> す</span><span style=\"background-color: #FFFDFD\"> 可</span><span style=\"background-color: #FFFDFD\"> 能</span><span style=\"background-color: #FFFCFC\"> 性</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFFCFC\"> あ</span><span style=\"background-color: #FFFAFA\"> る</span><span style=\"background-color: #FFEAEA\"> 事</span><span style=\"background-color: #FFDCDC\"> 項</span><span style=\"background-color: #FFFBFB\"> に</span><span style=\"background-color: #FFFDFD\"> は</span><span style=\"background-color: #FFFDFD\"> 、</span><span style=\"background-color: #FFF8F8\"> 以下</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFFAFA\"> ような</span><span style=\"background-color: #FFF9F9\"> もの</span><span style=\"background-color: #FFFEFE\"> が</span><span style=\"background-color: #FFF5F5\"> あり</span><span style=\"background-color: #FFDADA\"> ます</span><span style=\"background-color: #FFCCCC\"> 。</span><span style=\"background-color: #FFDFDF\"> なお</span><span style=\"background-color: #FFFDFD\"> 、</span><span style=\"background-color: #FFEBEB\"> [UNK]</span><span style=\"background-color: #FFFBFB\"> に</span><span style=\"background-color: #FFF9F9\"> おける</span><span style=\"background-color: #FFDFDF\"> 将来</span><span style=\"background-color: #FFFEFE\"> に</span><span style=\"background-color: #FFFBFB\"> 関する</span><span style=\"background-color: #FFD8D8\"> 事項</span><span style=\"background-color: #FFFAFA\"> は</span><span style=\"background-color: #FFFCFC\"> 、</span><span style=\"background-color: #FFF6F6\"> 当</span><span style=\"background-color: #FFF8F8\"> 事業</span><span style=\"background-color: #FFFBFB\"> 年度</span><span style=\"background-color: #FFFDFD\"> 末</span><span style=\"background-color: #FFEBEB\"> 現在</span><span style=\"background-color: #FFFEFE\"> に</span><span style=\"background-color: #FFFBFB\"> おいて</span><span style=\"background-color: #FFEBEB\"> 当社</span><span style=\"background-color: #FFFDFD\"> が</span><span style=\"background-color: #FFDBDB\"> 判断</span><span style=\"background-color: #FFE8E8\"> した</span><span style=\"background-color: #FFE6E6\"> もの</span><span style=\"background-color: #FFF7F7\"> であり</span><span style=\"background-color: #FFEBEB\"> ます</span><span style=\"background-color: #FFD3D3\"> 。</span><span style=\"background-color: #FFF6F6\"> （</span><span style=\"background-color: #FFF3F3\"> [UNK]</span><span style=\"background-color: #FFFBFB\"> ）</span><span style=\"background-color: #FFA3A3\"> 主要</span><span style=\"background-color: #FFAFAF\"> 市場</span><span style=\"background-color: #FFFCFC\"> で</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFD7D7\"> 需要</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFF4F4\"> 急激な</span><span style=\"background-color: #FF0000\"> 変動</span><span style=\"background-color: #FFA7A7\"> に</span><span style=\"background-color: #FFAAAA\"> ついて</span><span style=\"background-color: #FFEDED\"> 当社</span><span style=\"background-color: #FFF6F6\"> は</span><span style=\"background-color: #FFFDFD\"> 、</span><span style=\"background-color: #FFF4F4\"> 主に</span><span style=\"background-color: #FFE8E8\"> 半導体</span><span style=\"background-color: #FFF7F7\"> 業界</span><span style=\"background-color: #FFFDFD\"> 及び</span><span style=\"background-color: #FFF1F1\"> [UNK]</span><span style=\"background-color: #FFF6F6\"> 業界</span><span style=\"background-color: #FFFEFE\"> を</span><span style=\"background-color: #FFE5E5\"> 対象</span><span style=\"background-color: #FFFDFD\"> と</span><span style=\"background-color: #FFF2F2\"> して</span><span style=\"background-color: #FFFCFC\"> 、</span><span style=\"background-color: #FFFCFC\"> その</span><span style=\"background-color: #FFF7F7\"> 生産</span><span style=\"background-color: #FFF5F5\"> ライン</span><span style=\"background-color: #FFFBFB\"> で</span><span style=\"background-color: #FFC4C4\"> 用い</span><span style=\"background-color: #FFF9F9\"> られる</span><span style=\"background-color: #FFF9F9\"> 各種</span><span style=\"background-color: #FFE5E5\"> 生産</span><span style=\"background-color: #FFFBFB\"> 設備</span><span style=\"background-color: #FFE5E5\"> 部品</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFDCDC\"> 製造</span><span style=\"background-color: #FFFAFA\"> ・</span><span style=\"background-color: #FFF8F8\"> 販売</span><span style=\"background-color: #FFA4A4\"> を</span><span style=\"background-color: #FFFEFE\"> 行って</span><span style=\"background-color: #FFFDFD\"> い</span><span style=\"background-color: #FFC9C9\"> ます</span><span style=\"background-color: #FFFEFE\"> が</span><span style=\"background-color: #FFFBFB\"> 、</span><span style=\"background-color: #FFEFEF\"> 半導体</span><span style=\"background-color: #FFF7F7\"> 業界</span><span style=\"background-color: #FFFBFB\"> に</span><span style=\"background-color: #FFEBEB\"> おき</span><span style=\"background-color: #FFFDFD\"> まして</span><span style=\"background-color: #FFFEFE\"> シリコン</span><span style=\"background-color: #FFFDFD\"> サイクル</span><span style=\"background-color: #FFFDFD\"> 、</span><span style=\"background-color: #FFFBFB\"> [UNK]</span><span style=\"background-color: #FFF7F7\"> 業界</span><span style=\"background-color: #FFE6E6\"> に</span><span style=\"background-color: #FFC4C4\"> おき</span><span style=\"background-color: #FFE6E6\"> まして</span><span style=\"background-color: #FFFEFE\"> クリスタル</span><span style=\"background-color: #FFFDFD\"> サイクル</span><span style=\"background-color: #FFFAFA\"> と</span><span style=\"background-color: #FFFEFE\"> 呼ば</span><span style=\"background-color: #FFF9F9\"> れる</span><span style=\"background-color: #FFF4F4\"> 業界</span><span style=\"background-color: #FFF5F5\"> 特有の</span><span style=\"background-color: #FFF7F7\"> 好</span><span style=\"background-color: #FFFEFE\"> 不況</span><span style=\"background-color: #FFFBFB\"> の</span><span style=\"background-color: #FFFCFC\"> 波</span><span style=\"background-color: #FFFCFC\"> が</span><span style=\"background-color: #FFF5F5\"> 存在</span><span style=\"background-color: #FFF1F1\"> し</span><span style=\"background-color: #FFACAC\"> ます</span><span style=\"background-color: #FFFDFD\"> 。</span><span style=\"background-color: #FFDBDB\"> 当社</span><span style=\"background-color: #FFFBFB\"> に</span><span style=\"background-color: #FFF7F7\"> おき</span><span style=\"background-color: #FFF7F7\"> まして</span><span style=\"background-color: #FFFDFD\"> は</span><span style=\"background-color: #FFFCFC\"> 、</span><span style=\"background-color: #FFF1F1\"> メーカー</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFF9F9\"> 設備</span><span style=\"background-color: #FFFBFB\"> 投資</span><span style=\"background-color: #FFF3F3\"> 動向</span><span style=\"background-color: #FFFEFE\"> に</span><span style=\"background-color: #FFFBFB\"> 左右さ</span><span style=\"background-color: #FFFBFB\"> れ</span><span style=\"background-color: #FF5555\"> ない</span><span style=\"background-color: #FFFCFC\"> 消耗</span><span style=\"background-color: #FFFBFB\"> 品</span><span style=\"background-color: #FFFCFC\"> など</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFCBCB\"> 安定</span><span style=\"background-color: #FFFCFC\"> 的な</span><span style=\"background-color: #FFF6F6\"> 販売</span><span style=\"background-color: #FFFEFE\"> が</span><span style=\"background-color: #FFFDFD\"> [UNK]</span><span style=\"background-color: #FFFBFB\"> 分野</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFFEFE\"> 受注</span><span style=\"background-color: #FFFEFE\"> に</span><span style=\"background-color: #FFD3D3\"> 注力</span><span style=\"background-color: #FFF2F2\"> する</span><span style=\"background-color: #FFF8F8\"> など</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFF7F7\"> 対策</span><span style=\"background-color: #FFF4F4\"> を</span><span style=\"background-color: #FFFCFC\"> 行い</span><span style=\"background-color: #FFFCFC\"> 、</span><span style=\"background-color: #FFFBFB\"> 業績</span><span style=\"background-color: #FFECEC\"> へ</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FF9494\"> 影響</span><span style=\"background-color: #FFECEC\"> を</span><span style=\"background-color: #FFFCFC\"> 最小</span><span style=\"background-color: #FFFAFA\"> 限</span><span style=\"background-color: #FFFBFB\"> に</span><span style=\"background-color: #FFF4F4\"> す</span><span style=\"background-color: #FFFBFB\"> べく</span><span style=\"background-color: #FFF2F2\"> 努力</span><span style=\"background-color: #FFF3F3\"> して</span><span style=\"background-color: #FFFCFC\"> おり</span><span style=\"background-color: #FFF1F1\"> ます</span><span style=\"background-color: #FFFBFB\"> 。</span><span style=\"background-color: #FFF3F3\"> しかしながら</span><span style=\"background-color: #FFFDFD\"> 、</span><span style=\"background-color: #FFFAFA\"> これ</span><span style=\"background-color: #FFFDFD\"> ら</span><span style=\"background-color: #FFFDFD\"> の</span><span style=\"background-color: #FFFDFD\"> 景気</span><span style=\"background-color: #FFE8E8\"> 変動</span><span style=\"background-color: #FFFBFB\"> に</span><span style=\"background-color: #FFFCFC\"> よって</span><span style=\"background-color: #FFFDFD\"> 、</span><span style=\"background-color: #FF9292\"> 当社</span><span style=\"background-color: #FFFEFE\"> の</span><span style=\"background-color: #FFFAFA\"> 業績</span><span style=\"background-color: #FFEFEF\"> 及び</span><span style=\"background-color: #FFF8F8\"> 財務</span><span style=\"background-color: #FFF6F6\"> 状況</span><span style=\"background-color: #FFFAFA\"> に</span><span style=\"background-color: #FF3030\"> 影響</span><span style=\"background-color: #FFFCFC\"> を</span><span style=\"background-color: #FFF9F9\"> 与える</span><span style=\"background-color: #FFFDFD\"> 可能</span><span style=\"background-color: #FFFCFC\"> 性</span><span style=\"background-color: #FFFCFC\"> が</span><span style=\"background-color: #FFE0E0\"> あり</span><span style=\"background-color: #FFECEC\"> ます</span><span style=\"background-color: #FFFCFC\"> 。</span><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['当社', 'の', '業績', 'は', '多岐に', 'わたる', '変動', '要因', 'の', '影響', 'を', '受ける', '可能', '性', 'が', 'あり', 'ます', '。', '有価', '証券', '報告', '書', 'に', '記載', 'した', '事業', 'の', '状況', '、', '経理', 'の', '状況', '等', 'に', '関する', '事項', 'の', 'うち', '、', '投資', '者', 'の', '判断', 'に', '重要な', '影響', 'を', '及', 'ぼ', 'す', '可', '能', '性', 'の', 'あ', 'る', '事', '項', 'に', 'は', '、', '以下', 'の', 'ような', 'もの', 'が', 'あり', 'ます', '。', 'なお', '、', '[UNK]', 'に', 'おける', '将来', 'に', '関する', '事項', 'は', '、', '当', '事業', '年度', '末', '現在', 'に', 'おいて', '当社', 'が', '判断', 'した', 'もの', 'であり', 'ます', '。', '（', '[UNK]', '）', '主要', '市場', 'で', 'の', '需要', 'の', '急激な', '変動', 'に', 'ついて', '当社', 'は', '、', '主に', '半導体', '業界', '及び', '[UNK]', '業界', 'を', '対象', 'と', 'して', '、', 'その', '生産', 'ライン', 'で', '用い', 'られる', '各種', '生産', '設備', '部品', 'の', '製造', '・', '販売', 'を', '行って', 'い', 'ます', 'が', '、', '半導体', '業界', 'に', 'おき', 'まして', 'シリコン', 'サイクル', '、', '[UNK]', '業界', 'に', 'おき', 'まして', 'クリスタル', 'サイクル', 'と', '呼ば', 'れる', '業界', '特有の', '好', '不況', 'の', '波', 'が', '存在', 'し', 'ます', '。', '当社', 'に', 'おき', 'まして', 'は', '、', 'メーカー', 'の', '設備', '投資', '動向', 'に', '左右さ', 'れ', 'ない', '消耗', '品', 'など', 'の', '安定', '的な', '販売', 'が', '[UNK]', '分野', 'の', '受注', 'に', '注力', 'する', 'など', 'の', '対策', 'を', '行い', '、', '業績', 'へ', 'の', '影響', 'を', '最小', '限', 'に', 'す', 'べく', '努力', 'して', 'おり', 'ます', '。', 'しかしながら', '、', 'これ', 'ら', 'の', '景気', '変動', 'に', 'よって', '、', '当社', 'の', '業績', '及び', '財務', '状況', 'に', '影響', 'を', '与える', '可能', '性', 'が', 'あり', 'ます', '。']\n",
      "[2, 11466, 5, 5356, 9, 10988, 5032, 5577, 4486, 5, 343, 10, 1164, 446, 108, 11, 125, 1953, 7, 27042, 4190, 1036, 520, 8, 1427, 20, 318, 5, 919, 6, 18270, 5, 919, 174, 8, 492, 6109, 5, 381, 6, 2869, 57, 5, 1843, 8, 1186, 343, 10, 14757, 7988, 954, 4261, 4636, 108, 5, 1208, 3282, 270, 1235, 8, 9, 6, 200, 5, 255, 60, 11, 125, 1953, 7, 194, 6, 1, 8, 217, 3253, 8, 492, 6109, 9, 6, 719, 318, 594, 562, 95, 8, 117, 11466, 11, 1843, 20, 60, 103, 1953, 7, 16, 1, 17, 1681, 1344, 13, 5, 3988, 5, 16137, 5577, 8, 130, 11466, 9, 6, 542, 10992, 3197, 388, 1, 3197, 10, 703, 12, 19, 6, 58, 506, 1364, 13, 519, 124, 3181, 506, 2219, 3905, 5, 487, 15, 409, 10, 462, 142, 1953, 11, 6, 10992, 3197, 8, 7779, 26207, 23571, 7372, 6, 1, 3197, 8, 7779, 26207, 12734, 7372, 12, 180, 41, 3197, 9453, 2542, 11958, 5, 1639, 11, 162, 31, 1953, 7, 11466, 8, 7779, 26207, 9, 6, 1327, 5, 2219, 2869, 9824, 8, 19707, 54, 47, 15016, 695, 42, 5, 1847, 96, 409, 11, 1, 1300, 5, 13390, 8, 20182, 22, 42, 5, 2015, 10, 845, 6, 5356, 61, 5, 343, 10, 6983, 5682, 8, 954, 3011, 5693, 19, 79, 1953, 7, 3276, 6, 73, 94, 5, 9987, 5577, 8, 86, 6, 11466, 5, 5356, 388, 5540, 919, 8, 343, 10, 2377, 446, 108, 11, 125, 1953, 7, 3]\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "from utils.predict import predict2\n",
    "\n",
    "input_text = \"当社の業績は多岐にわたる変動要因の影響を受ける可能性があります。有価証券報告書に記載した事業の状況、経理の状況等に関する事項のうち、投資者の判断に重要な影響を及ぼす可能性のある事項には、以下のようなものがあります。なお、文中における将来に関する事項は、当事業年度末現在において当社が判断したものであります。（１）主要市場での需要の急激な変動について当社は、主に半導体業界及びＦＰＤ業界を対象として、その生産ラインで用いられる各種生産設備部品の製造・販売を行っていますが、半導体業界におきましてシリコンサイクル、ＦＰＤ業界におきましてクリスタルサイクルと呼ばれる業界特有の好不況の波が存在します。当社におきましては、メーカーの設備投資動向に左右されない消耗品などの安定的な販売が見込める分野の受注に注力するなどの対策を行い、業績への影響を最小限にすべく努力しております。しかしながら、これらの景気変動によって、当社の業績及び財務状況に影響を与える可能性があります。\"\n",
    "#net_trained = build_bert_model()\n",
    "net_trained.eval()\n",
    "html_output = predict(input_text, net_trained)\n",
    "print(\"======================推論結果の表示======================\")\n",
    "print(input_text)\n",
    "display(HTML(html_output))\n",
    "\n",
    "pred_output = predict2(input_text, net_trained)\n",
    "print(pred_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66f6209d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['当社', 'の', '業績', 'は', '多岐に', 'わたる', '変動', '要因', 'の', '影響', 'を', '受ける', '可能', '性', 'が', 'あり', 'ます', '。', '有価', '証券', '報告', '書', 'に', '記載', 'した', '事業', 'の', '状況', '、', '経理', 'の', '状況', '等', 'に', '関する', '事項', 'の', 'うち', '、', '投資', '者', 'の', '判断', 'に', '重要な', '影響', 'を', '及', 'ぼ', 'す', '可', '能', '性', 'の', 'あ', 'る', '事', '項', 'に', 'は', '、', '以下', 'の', 'ような', 'もの', 'が', 'あり', 'ます', '。', 'なお', '、', '[UNK]', 'に', 'おける', '将来', 'に', '関する', '事項', 'は', '、', '当', '事業', '年度', '末', '現在', 'に', 'おいて', '当社', 'が', '判断', 'した', 'もの', 'であり', 'ます', '。', '（', '[UNK]', '）', '主要', '市場', 'で', 'の', '需要', 'の', '急激な', '変動', 'に', 'ついて', '当社', 'は', '、', '主に', '半導体', '業界', '及び', '[UNK]', '業界', 'を', '対象', 'と', 'して', '、', 'その', '生産', 'ライン', 'で', '用い', 'られる', '各種', '生産', '設備', '部品', 'の', '製造', '・', '販売', 'を', '行って', 'い', 'ます', 'が', '、', '半導体', '業界', 'に', 'おき', 'まして', 'シリコン', 'サイクル', '、', '[UNK]', '業界', 'に', 'おき', 'まして', 'クリスタル', 'サイクル', 'と', '呼ば', 'れる', '業界', '特有の', '好', '不況', 'の', '波', 'が', '存在', 'し', 'ます', '。', '当社', 'に', 'おき', 'まして', 'は', '、', 'メーカー', 'の', '設備', '投資', '動向', 'に', '左右さ', 'れ', 'ない', '消耗', '品', 'など', 'の', '安定', '的な', '販売', 'が', '[UNK]', '分野', 'の', '受注', 'に', '注力', 'する', 'など', 'の', '対策', 'を', '行い', '、', '業績', 'へ', 'の', '影響', 'を', '最小', '限', 'に', 'す', 'べく', '努力', 'して', 'おり', 'ます', '。', 'しかしながら', '、', 'これ', 'ら', 'の', '景気', '変動', 'に', 'よって', '、', '当社', 'の', '業績', '及び', '財務', '状況', 'に', '影響', 'を', '与える', '可能', '性', 'が', 'あり', 'ます', '。']\n",
      "[2, 11466, 5, 5356, 9, 10988, 5032, 5577, 4486, 5, 343, 10, 1164, 446, 108, 11, 125, 1953, 7, 27042, 4190, 1036, 520, 8, 1427, 20, 318, 5, 919, 6, 18270, 5, 919, 174, 8, 492, 6109, 5, 381, 6, 2869, 57, 5, 1843, 8, 1186, 343, 10, 14757, 7988, 954, 4261, 4636, 108, 5, 1208, 3282, 270, 1235, 8, 9, 6, 200, 5, 255, 60, 11, 125, 1953, 7, 194, 6, 1, 8, 217, 3253, 8, 492, 6109, 9, 6, 719, 318, 594, 562, 95, 8, 117, 11466, 11, 1843, 20, 60, 103, 1953, 7, 16, 1, 17, 1681, 1344, 13, 5, 3988, 5, 16137, 5577, 8, 130, 11466, 9, 6, 542, 10992, 3197, 388, 1, 3197, 10, 703, 12, 19, 6, 58, 506, 1364, 13, 519, 124, 3181, 506, 2219, 3905, 5, 487, 15, 409, 10, 462, 142, 1953, 11, 6, 10992, 3197, 8, 7779, 26207, 23571, 7372, 6, 1, 3197, 8, 7779, 26207, 12734, 7372, 12, 180, 41, 3197, 9453, 2542, 11958, 5, 1639, 11, 162, 31, 1953, 7, 11466, 8, 7779, 26207, 9, 6, 1327, 5, 2219, 2869, 9824, 8, 19707, 54, 47, 15016, 695, 42, 5, 1847, 96, 409, 11, 1, 1300, 5, 13390, 8, 20182, 22, 42, 5, 2015, 10, 845, 6, 5356, 61, 5, 343, 10, 6983, 5682, 8, 954, 3011, 5693, 19, 79, 1953, 7, 3276, 6, 73, 94, 5, 9987, 5577, 8, 86, 6, 11466, 5, 5356, 388, 5540, 919, 8, 343, 10, 2377, 446, 108, 11, 125, 1953, 7, 3]\n",
      "['財務', '諸', '表', 'に', 'ついて', 'は', '、', '金融', '商品', '取引', '法', '第', '[UNK]', '条', 'の', '[UNK]', '第', '[UNK]', '項', 'の', '規定', 'に', '基づき', '、', '三', '優', '監査', '法人', 'の', '監査', 'を', '受けて', 'おり', 'ます', 'が', '、', '製品', '分野', '別', '売上高', 'に', 'ついて', 'は', '、', '当該', '監査', 'を', '受けて', 'おり', 'ませ', 'ん', '。']\n",
      "[2, 5540, 1108, 1211, 8, 130, 9, 6, 2574, 1429, 2246, 202, 55, 1, 724, 5, 1, 55, 1, 1235, 5, 1333, 8, 4157, 6, 168, 4709, 7485, 907, 5, 7485, 10, 613, 79, 1953, 11, 6, 1151, 1300, 1050, 16720, 8, 130, 9, 6, 4306, 7485, 10, 613, 79, 5904, 1357, 7, 3]\n",
      "['財産', '権', '等', 'に', 'ついて', '当社', 'は', '、', '他', '社', 'の', '特許', '権', '等', 'の', '知的', '財産', '権', 'を', '[UNK]', 'ない', 'よう', '[UNK]', '注意', 'を', '払い', '、', '受注', 'と', '技術', '開発', 'に', 'あたって', 'い', 'ます', 'が', '、', '第', '三', '者', 'の', '特許', '権', '等', 'の', '知的', '財産', '権', 'を', '侵害', 'する', 'と', 'して', '損害', '賠償', '等', 'の', '請求', 'を', '受ける', '可能', '性', 'が', 'あり', 'ます', '。']\n",
      "[2, 4347, 193, 174, 8, 130, 11466, 9, 6, 151, 173, 5, 4680, 193, 174, 5, 9312, 4347, 193, 10, 1, 47, 762, 1, 2665, 10, 9640, 6, 13390, 12, 442, 203, 8, 5064, 142, 1953, 11, 6, 55, 168, 57, 5, 4680, 193, 174, 5, 9312, 4347, 193, 10, 9672, 22, 12, 19, 4262, 6666, 174, 5, 5111, 10, 1164, 446, 108, 11, 125, 1953, 7, 3]\n",
      "['医療', '機器', 'の', '法的', '規制', '等', 'に', 'ついて', '当社', 'は', '今後', '医療', '機器', 'の', '販売', 'を', '予定', 'して', 'おり', 'ます', 'が', '、', '医療', '機器', 'は', '患者', 'の', '生命', '及び', '[UNK]', '影響', 'を', '及', 'ぼ', 'す', '可', '能', '性', 'が', 'あ', 'る', 'た', 'め', '、', '品質', 'の', '[UNK]', '保持', '、', '医療', '現場', 'に', 'おける', '正しい', '方法', 'で', 'の', '使用', 'が', '求め', 'られる', 'こと', 'から', '、', '「', '医薬', '品', '、', '医療', '機器', '等', 'の', '品質', '、', '有効', '性', '及び', '安全', '性', 'の', '確保', '等', 'に', '関する', '法律', '」', 'に', 'おいて', '、', '製造', 'から', '販売', 'に', '至る', '各', '流通', '過程', 'で', 'の', '遵守', '事項', 'が', '定め', 'られて', 'おり', 'ます', '。']\n",
      "[2, 1657, 2084, 5, 9762, 2576, 174, 8, 130, 11466, 9, 4289, 1657, 2084, 5, 409, 10, 685, 19, 79, 1953, 11, 6, 1657, 2084, 9, 2984, 5, 3189, 388, 1, 343, 10, 14757, 7988, 954, 4261, 4636, 108, 11, 1208, 3282, 1426, 3202, 6, 5939, 5, 1, 3301, 6, 1657, 3096, 8, 217, 6925, 771, 13, 5, 153, 11, 2507, 124, 30, 27, 6, 24, 6216, 695, 6, 1657, 2084, 174, 5, 5939, 6, 4165, 108, 388, 1629, 108, 5, 2005, 174, 8, 492, 1442, 25, 8, 117, 6, 487, 27, 409, 8, 1372, 286, 3804, 2767, 13, 5, 21901, 6109, 11, 2525, 157, 79, 1953, 7, 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils.predict import predict2\n",
    "\n",
    "df = pd.read_csv(\"/Users/chenying/testsentiment.csv\", encoding=\"utf-8-sig\")\n",
    "df[\"PREDICT\"] = np.nan   #予測列を追加\n",
    "\n",
    "net_trained.eval()  #推論モードに。\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, \"PREDICT\"] = predict2(row['text'], net_trained.numpy()[0]  # GPU環境の場合は「.cpu().numpy()」としてください。\n",
    "\n",
    "#print(df)    \n",
    "df.to_csv(\"/Users/chenying/predicted_test .csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48adcea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
